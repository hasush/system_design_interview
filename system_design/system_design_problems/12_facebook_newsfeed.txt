* Let's design Facebook's Newsfeed which would contain posts, photos, videos
  and status updates


* What is Facebook's Newfeed?
    - A NewsFeed is the constantly updating list of stories in the middle of Facebook's 
      homepage.
    - It includes status updates, photos, videos, links, app activity, and 'likes' 
      from people, pages, and groups that a user follows on Facebook. 
    - In other words, it is a compilation of a complete scrollable version of your 
      friends' and your life story from photos, videos, locations, status updates,
      and other activities. 
    - For any social media site you design, i.e. Twitter, Instagram, or Facebook 
      you will need some news feed system to display updates from friends and followers 


* Requirements and Goals of the System 
    - Functional Requirements 
        * Newsfeed will be generated based on the posts from the people, pages, and
          groups that a user follows.
        * A user may have many friends and follow a large number of pages/groups 
        * Feed may contain images, videos, or just text 
        * Our service should support appending new posts as they arrive 
          to the newsfeed for all active users 
    - Non-functional requirements 
        * Our system should be able to generate any user's newfeed in real time 
          Maximum latency seen by end user would be 2s 
        * A post shouldn't take more than 5s to make it to a user's feed assuming 
          a new newsfeed request comes in.


* Capacity Estimation and Constraints 
    - Let's assume on average a user has 300 friends and follows 200 pages 
    - Traffic Estimates 
        * Let's assume 300e6 DAU with each user fetching their timeline on 
          average of fives times per day 
            300e6 * 5 = 1.5e9 newsfeed requests/day = 1.5e9/(24*3600)<150e7/(75e3)=2e4 requets/second 
    - Storage Estimates 
        * On average, let's assume we need to have around 500 posts in every user's 
          feed that we want to keep in memory for a quick fetch. 
        * Assume on average each post would be 1 KB in size
        * Need roughly 500 kB of data/user 
        * 300e6*500 kB = 150e12 = 150 TB of memory 
        * If a server holds 100 GB, we would need 1500 machines to keep the 
          the top 500 posts in memory for all active users.


* System APIs
    - We can have SOAP or REST APIs to expose the functionality of our service. 
    - getUserFeed(api_dev_key, user_id, since_id, count, max_id, exclude_replieS)
        * api_dev_key (string): The API developer key of a registered can be used to, 
          among other things, throttle users based on their allocated quota.
        * user_id (number): The ID of the user for whom the system will generate the 
          newsfeed.
        * since_id (number): Optional; returns results with an ID higher than 
          (that is, more recent than) the specified ID.
        * count (number): Optional; specifies the number of feed items to try and 
          retrieve up to a maximum of 200 per distinct request.
        * max_id (number): Optional; returns results with an ID less than 
          (that is, older than) or equal to the specified ID.
        * exclude_replies(boolean): Optional; this parameter will prevent replies 
          from appearing in the returned timeline.
        * Returns: (JSON) Returns a JSON object containing a list of feed items.


* Database Design 
    - There are three primary objects 
        * User
        * Entity (page, group, etc.)
        * FeedItem 
    - Relationships between primary objects 
        * A User can follow other entities and can become friends with other users 
        * Both users and entities can post FeedItems which can contain
          text,images, or videos 
        * Each FeedItem will have a UserID which will point to the User who created it.
        * For simplicity, let's assume that only users can create feed items, altough 
          Facebook Pages can post feed items too.
        * Each FeedItem can optionally have an EntityID pointing to the page 
          or group where that post was created. 
    - If we are using realational database, we would need to model two relations 
        * User-Entity Relation 
        * FeedItem-Media Relation 
    - Since each user can be friends with many people and follow a lot of entities,
      we can store this relation in a separate table. 
    - The "Type" column in "UserFollow" identifies if the entity being followed 
      is a user or Entity. Similarly, we can have a table for FeedMedia relation. 


    User                            Entity                          UserFollow 
PK  UserID: int                 PK  EntityID: int               PK  UserID: int 
    Name: varchar(20)               Type: smallint              PK  EntityOrFriendID: int 
    Email: varchar(32)              Description: varchar(512)       Type: smallint 
    DOB: datetime                   CreationDate: datetime 
    CreationDate: datetime          Category: smallint 
    LastLogin: datetime             Phone: varchar(12)
                                    Email: varchar(20)
    
    FeedItem                        Media                           FeedMedia 
PK  FeedItemID: int             PK  MediaID: int                PK  FeedItemID: int 
    UserID: int                     Type: smallint                  MediaID: int 
    Contents: varchar(256)          Description: varchar(256)
    EntityID: int                   Path: varchar(256)
    Latitude: int                   Latitude: int 
    Longitude: int                  Longitude: int 
    CreationDate: datetime          CreationDate: datetime 
    NumLikes: int 


* High Level System Design 
    - At high level, system divided into two parts
        * Feed Generation 
        * Feed Publishing 
    - Feed Generation 
        * Newsfeed is generated from the posts (or feed items) of users and entities 
          (pages and groups) that a user follows. 
        * Whenever our system receives a request to generate the feed for a user, we 
          will perform the following steps
            - Retreive IDs of all users and entities that users follows
            - Retreive latest, most popular and relevant posts for those IDs 
              These are the potential posts we will show in the user's NewsFeed.
            - Rank these posts based on the relevance to user. This represents 
              user's current feed. 
            - Store this feed in the cache and return top posts (say 20) to be rendered 
              in user's feed. 
            - On the front end, when user reaches the end of the current feed, the 
              next 20 posts will be fetched from the server 
        * One thing to notice here is that we generated the feed once and stored it in 
          cache. 
        * What about new incoming posts from people that the user follows?
        * If the user is online, we should have a mechanism to rank and add those new 
          posts to the feed.
        * We can periodically, say every five minutes, perform the above steps to rank 
          and add the newer posts to the feed.
        * The user can be notified that there are newer items in the feed that can be fetched. 
    - Feed Publishing 
        * Whenever the user loads the newsfeed page, the user has to request and pull 
          feed items from the server. 
        * When the user reaches the end of the current feed, they can pull more 
          data from the server. 
        * For newer items either the server can notify the user and then they can pull, 
          or the server can push these new posts. 
        * At a high level, we will need the following components in our system 
            - Web Servers 
                * To maintain a connection with the user.
                * This connection will be used to transfer data between the user 
                  and the server. 
            - Application Server 
                * To execute workflows of storing new posts in the database servers. 
                * We will also need some application servers to retrieve and push the
                  newsfeed to the end user. 
            - Metadata database and cache 
                * To store the metadata about Users, Pages, and Groups 
            - Posts database and cache 
                * To store metadat about posts and their contents 
            - Video and photo storage, and cache 
                * Blob storage, to store all the media included in the posts 
            - Newsfeed generation service 
                * To gather and rank all the relevant posts for a user to generate 
                  newsfeed and store in the cache. 
                * The service will also receive live updates and will add these 
                  newer feed items to any user's timeline 
            - Feed notification service 
                * To notify the user that there are newer items available for their 
                  newsfeed 
    - See figure facebook_newsfeed_figure1.png 

                                                         |---------------------------------------------------------|
                                                         v                                                         |
    user_a --- add/update post ---> web server ---> application server ---> metadata cache <---> metadata DB       |
                                                         |                  ^                                      |
                                                         |                  |        ------------------------------|
                                                         |                  |        |                             |
                                                         v                  |        V                             |
                                            news feed generation service <--|---> posts cache <----> posts db      |
                                                         |                  |                                      |
                                                         |                  |       |------------------------------|
                                                         |                  |       V
                                                         |                  |---> media cache <----> video/photo storage 
                                                         |
                                                         v
                        feed notification server ---> cache servers holding feed
                                    |                        ^
                                    |                        |
                                    |                        |
                                    |                        |
                                    |                        |
                                    v                        v
    user_b <---feed update --->  web_server ---get feed ---> application server 
    user_c <---feed update --->


* Detailed Component Design 
    - Let's discuss different componenets of system in detail 
    - Feed Generation 
        * Lets take a simple case of the newsfeed generation service fetching most recent posts from all the users 
          and enities that the user follows; the query would look like 

            (SELECT FeedItemID FROM FeedItem WHERE UserID in (
            SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <current_user_id> and type = 0(user))
            )
            UNION
            (SELECT FeedItemID FROM FeedItem WHERE EntityID in (
                SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <current_user_id> and type = 1(entity))
            )
            ORDER BY CreationDate DESC 
            LIMIT 100

            - Issues with this design for feed generation service 
                * Crazy slow for users with a lot of friends/follows as we have to perform sorting/merging/ranking 
                  of a huge number of posts 
                * We generate the timeline when a user loads their page. This would be quite slow and have a high 
                  latency. 
                * For live updates, each status update will result in feed updates for all followers. This result in a 
                  high backlogs in our Newsfeed Generation Service. 
                * For live updates, the server pushing (or notifying about) newer posts to users could lead to 
                  very ehavy loads, especially for people or pages that have a lot of followers.
                  To primve the effiency, we pre-generate the timeline and store it in memory. 
        * Offline generation for newsfeed 
            - We can have a dedicated servers that are continuously generating user's newfeed and stores 
              them in memory. 
                * So whenever a user requests for the new posts for their feed, we can simply serve it from the 
                  pregenerated, stored location. 
                * Using this scheme, user's newsfeed is not compiled on load, but rather on a regular basis and returned 
                  to users whenever they request for it. 
            - Whenever these servers need to generate the feed for a user, they will first query to see when was the 
              last time the feed was generated for that user. 
                * Then, the new feed data would be generated from that tiem onwards. 
                * We can store this data in a hash table where the "key" would be UserID and the "value" would be a 
                  STRUCT like this:
                    - Struct {
                          LinkedHashMap<FeedItemID, FeedItem> feedItems;
                          DateTime lastGenerated;
                      }
                * We can store FeedItemIDs in a data structure similar to LinkedHashMap or TreeMap which can 
                  allow us to not only jump to any feed item but also iterate through the map easily.
                * Whenever users want to fetch more feed items, they can send the last FeedItemID they currently
                  see in their newsfeed, we can then jump to that FeedItemID in our hash-map and return 
                  next batch/page of feed items from there. 
            - How many feed items should we store in memory for a user's feed?
                * Initially, we can decide to store 500 feed items per user,
                  but this number can be adjusted later based on the usage pattern.
                * For example, if we assume that one page of a user's feed has 20 
                  posts and most of the users never browse more than ten pages for their 
                  feed, we can decide to store only 200 posts per user. 
                * For any user who wants to see more posts (more than what is stored 
                  in memory), we can always query backend servers. 
            - Should we generate (and keep in memory) newsfeeds for all users?
                * There will be a lot of users that don't log-in frequently.
                * Here are a few things we can do to handle this
                    - A straightforward appraoch could be to use an LRU based 
                      cache that can remove users from memory that haven't accessed their
                      newsfeed for a long time 
                    - A smarter solution can figure out the login pattern of users to 
                      pre-generate their newsfeed, i.e. at waht time of the day a user is active 
                      and which days of the week does a user access their newsfeed, etc. 
    
    - Feed Publishing (live generation problem)               
        * The process of pushing a post to all the followers is called fanout.
        * By analogy, the push approach is called fanout-on-write, while the pull 
          approach is called fanout-on-load.
        * Let's discuss different options for publishing feed data to users. 
            - "Pull" model or Fan-out-on-load 
                * This method involves keeping all the recent feed data in memory 
                  so that users can pull it from the server wheenver they need it. 
                * Clients can pull the feed data on a regular basis or manually 
                  wheenver they need it. 
                * Possible Problems
                    - New data might not be shown to the users until they issue 
                      a pull request 
                    -  It's hard to find the right pull cadence, as most of the time pull
                       requests will result in an empty response if there is no new data 
                       causing a waste of resources 
            - "Push" model or Fan-out-on-write 
                * For a push system, once a user has published a post, we can immediately 
                  push this post to all the followers. 
                * The advantage is that when fetching feed you don't need to
                  go through your friend's list and gets feed for each of them. 
                * It significantly reduces read operations. 
                * To efficiently handle this, users have to maintain a Long Poll request 
                  with the server for receiving the updates. 
                * A possible problem with this approach is that when a user has millions 
                  of followers (a celebrity-user) the server has to push updates to a lot of 
                  people. 
            - Hybrid 
                * An alternate method to handel feed data could be to use a hybrid approach,
                  i.e. to do a combinations of fan-out-on-write and fan-out-on-load 
                * Specifically, we can stop pushing posts from users with a high number of followers 
                  (a celebrity user) and only push data for those users who have a few hundred 
                  (or thousand) followers.
                * For celebrity users, we can let the followers pull the updates since the push
                  operation can be extremely costly for users who have a lot of friends or followers.
                  By disabling fanout for them, we can save a huge number of resources. 
                * Another alternate approach could be that, once a user publishes a post,
                  we can limit the  fanout to only the user's online friends. 
                * Also, to get benefits from both the approaches, a combination of
                  'push to notify' and 'pull for serving' end-users is a great way to 
                  go. Purely a push or pull model is less versatile. 
    
    - How many feed items can we return to the client in each request?
        * We should have a maximum limit for the number of items a user can fetch in
          one request (say 20). But, we should let the client specify how many 
          feed items they want with each request as the user may like to fetch 
          a different number of posts depending on the device (mobile vs. desktop)
    - Should we always notify users if there are new posts available for their 
      newsfeed?
        * It could be useful for users to get notified wehenever new data is available.
          However, on mobile devices, where data usage is relatively expensive, it can 
          consume unnecessary bandwidth. Hence at for mobile devices, we can choose 
          not to push data, instead let users "pull to refresh" to get new posts. 

        
* Feed Ranking 
    - The most straightforward way to rank posts in a newsfeed is by the creation time 
      of the posts, but today's ranking algorithms are doing a lot more than that 
      to ensure "important" posts are ranked higher.
    - The high-level idea of ranking is first to select "key" signals that make a 
      post important and then to find out how to combine them to calculate a final 
      ranking score. 
    - More specifically, we can select features that are relevant to the importance of any 
      feed item, i.e. number of likes, comments, shares, time of the update, whether 
      the post has images/videos, etc. and then a score can be calculated using these 
      features.
    - This is generally enough for a simple ranking system. 
    - A better ranking system can significantly improve itself by constantly 
      evaluating if we are making progress in user stickiness, retention, ad revenue, etc. 


* Data Partitioning 
    - Sharding posts and metadata 
        * Since we have a huge number of new posts every day and our read load 
          is extremely high too, we need to distribute our data onto multiple 
          machines such that we can read/write it efficiently.
        * For sharding our databses that are storing posts and their metadata, we can have a 
          a similar desing as discussed in 6_twitter.txt 
    - Sharding feed data 
        * For feed data, which is being stored in memory, we can partition it based 
          on UserID.
        * We can try storing all the data of au ser on one server. 
        * When storing, we can pass the UserID to our hash function that will map 
          the user to cache server where we will store the user's feed objects. 
        * Also, for a given user, since we don't expect to store more than 500
          FeedItemIDs, we will not run into a scenario where feed data for a user 
          doesn't fit on a single server. 
        * To get the feed of a user, we would always have to query one only server.
        * For future growth and replications, we must use consistent hashing. 

                  



