* Design file hosting service like Dropbox or Google Drive
    - Cloud file storage enables users to store their data on remote servers
    - These storages are maintained by cload storage providers and made 
      available to users over a network.
    - Users pay for their cloud storage on a monthly basis 


* Why cloud storage?
    - Cloud storage services are being increasingly used as they simply storage 
      and the exchange of resources amongst multile devices.
    - The shift from single personal computer to multiple devices (phones,
      tablets, PCs) that share and utilize the same data is believed to be 
      one of the primary accelerations of cloud storage adoption.
    - Availability 
        * Have data available anywhere, anytime, and on any device. 
    - Reliability and Durability 
        * Offers 100% reliabilitiy and durabilitiy. Data will always be there 
          and non corrupted. Accomplishesed through having multiple redudant 
          copies spread out across the world. 
    - Scalabilitiy
        * Have unlimited storage (at a cost)
    

* Requirements and Goals of the System 
    - Users should be able to upload and download their files/photos 
      from any device. 
    - Users should be able to share files or folders with other users. 
    - Service should support automatic synchronization between devices.
      i.e. after updating a file on one device, it should be synchronized 
      on all devices. 
    - System should support storage of files up to a GB 
    - ACID compliance (Atomiticity, Consistency, Isolation, Durability)
      must be guaranteed for all files 
    - Service should support offline file editing. Users should be able to
      add/delete/modify files while offline, and as soon as they come online,
      all their changes should be synced to the remote server and other online devices.
    - Extended requirements:
        * snapshot of the data so that users can go back to any version of the files 


* Some design considerations 
    - Should expect huge read and write volumes 
    - Read/Write ratio should be nearly the same 
    - Interally, files can be saved in 4 MB chunks
        * Allows for better effiency, i.e. if a chunk had a problem with being 
          loaded, then just try to reload the chunk ... if a chunk did 
          not properly get uploaded ... only that chunk needs to be transferred 
          again 
        * Can reduce the amount of data exchange by transferring updated 
          chunks only 
        * By removing duplicated chunks, we can save storage sapce and bandwidth
          usage. 
        * Keeping a local copy of the metadata (file name, size, etc.) with the client 
          can save us a lot of round trips to the server. 
        * For small changes, clients can intelligently upload the diffs instead of 
          the whole chunk. 


* Capacity Estimation and Constraints 
    - Assume 500e6 total users and 100e6 daily users 
    - On average, each user connects from 3 devices 
    - Assume on average 200files/photos --> 5e8*2e*2=100 billion total files 
    - Assume average file size is 100kB --> 1e5*1e11=1e16 = 10 PB storage 
    - Assume 1e6 active connections per minute 

* High Level Design 
    - The user will specify a folder as the workspace on their device.
    - Any file/photo/folder placed in this folder will be uploaded to the 
      cloud, and whenever a file is modified or deleted, it will be 
      reflected in the same way in the cloud storage. 
    - User can specify similar workspaces on any device. Changes in the workspace
      on one device will be propagated to the cloud and all other devices so that 
      the same view of the workspace exists.
    - At a high level, we need to store files and their metadata information.
        * File Name, File Size, Directory, etc. and who file is shared with 
        * Need servers that can help the clients to upload/download files
          to cloud Storage 
        * Need servers can facilidate updating metadata about files and users 
        * Also need some mechanisms to notify all clients whenever an update 
          happens so that all files can be synchorized 
        * As shown in diagram below, Block server will work with the clients
          to upload/download files from cloud storage and Metadata servers 
          will keep metadata of files updated in SQL or NoSQL databse.
        * Synchronization servers will handle the workflow of notifying 
          all clients about different changes for synchronization


    
      |---------------> block server -----------> cloud storage 
      |                      |
      |                      |-------------------> metadata storage 
      |                                                  ^
      |                                                  |
      |                                                  |
    client -----------> metadata server ------------------
      ^                         |
      |                         |
      |                         |
      |                         |
      |                         v
      |---------------- synchronization server   


* Component Design 

    - Client 
        * Client application monitors the workspace folder on the user's 
          machine and syncs all files/folders in it with the remote cloud storage 
        * Client application will work with storage servers to upload/download/modify 
          actual files to backend cloud storage. 
        * Client also interacts with the remote synchronization service to  handle
          and file metadata updates; i.e. change in file name, size modification date 
        * Essential Operations for the client
            - Upload and download files 
            - Detect file changes in the workspace folder 
            - Handle conflict due to offline or concurrent updates 
        * How do we handle file transfer efficiently?
            - We can break each file into smaller chunks so that we transfer only 
              those chunks that are modified and not the whole file. 
            - i.e. divide file into 4MB chunks. 
            - Could statically calculate what could be an optimal chunk size based on
                * Storage devices we use in the cloud to optimize space utilization 
                  and input/output operations per second (IOPS)
                * Network bandwidth 
                * Average filesize in the storage, etc.
                * In metadata, we should also keep a record of each file and the chunks
                  that constitute it.
        * Should we keep a copy of metadata with client?
            - Allows us to offline updates and also save trips to remote metadata 
        * How can clients efficiently listen to changes happening with other clients?
            - One solution would be clients periodically checking with server 
              to see if any changes.
                * Problem would be too many server requests and a delay in changes 
                  being reflected for the client 
            - Better solution would be HTTP long polling 
                * Client asks information from server with the expectation that 
                  the server might not respond immediately.
                * If server has no request to give when the poll is received, the 
                  server holds the request open and waits for response information 
                  to become available. 
                * Onces server has new information, the server immediately sends 
                  an HTTP/S response to the client, completing the open HTTP/S request
                * Based on the above considerations, we can divide our client into 
                  the following four parts:
                    - Internal metadata databse
                        * Will keep track of all the files, chunks, revisions,
                          and location within the file system.
                    - Chunker
                        * Will split the files into smaller pieces called chunks.
                        * Also responsible for reconstructing file from chunks.
                        * Chunking algorithm will keep track of the parts of the 
                          file that have been modified by the user and will only 
                          transfer those parts to the Cloud Storage
                          --> Saves bandwidth and synchronization time 
                    - Watcher 
                        * Will monitor the local worksapce folders and notify the 
                          Indexer of any action performed by the users
                            - i.e. when user creats, deletes, update files/folders 
                        * Also listens to any changes happening on other clients that
                          are broadcasted by synchronization service 
                    - Indexer 
                        * Will process the events received from the watcher and update the 
                          internal mettadata database with information about the chunks 
                          of the modified files.
                        * Once chunks are succssfully submitted/downloaded to the Cloud 
                          Storage, the Indexer will communicate with the remote Synchronization
                          Service to broadcast changes to other clients and update remote
                          metadata database. 


                            Client             <-------Data Flow -------> Block/Cloud Storage 
                        Chunker Indexer        
                        Watcher Internal DB    <-------Control Flow ----> Synchronization Service 
                                                                                    ^
                                                                                    |
                                                                                    |
                                                                                    |
                                                                                    |
                                                                                    v
                                                                               metadata DB

                * How should clients handle slow servers?
                    - Clients should exponentially back off if the server is busy/not responding
                    - I.E. if server is not responding ... increase time between next request exponetially 
                      with respect to current request 
                * Should mobile clients sync remote changes immediately? 
                    - Unlike desktop or web clients, mobile clients usually sync on demand to
                      save user's bandwidth and space 

    - Metadata Databse 
        * Responsible for maintaining the versioning and metadata information about
          files/chunks, users, and workspaces 
        * Can be relational or not, i.e. MySQL or a NoSQL database such as DynanmoDB 
        * Regardless of type of databse, the Synchornization Service should be able to 
          provide a consistent view of the files using a database, especially if
          more than one user is working with the same file simultaneously 
        * Since NoSQL databases do not support ACID compliance for the sake of 
          scalabilitiy and performance, ACID properties would need to be programmed
          into the logic of Sychronization service in case we opt for this kind of database.
        * Using relational database can simplify the implementation of the
          sychronization service as they natively support ACID properties
        * Information to store:
            - chunks
            - files
            - user
            - devices 
            - workspace (sync folders)

    - Synchronization Service 
        * Component that processes file updates by the client and applies these 
          changes to other subscribed clients. 
            - Sychronizes clients local databases with information stored in the Metadata DB
            - Most critical componenet of system architecture due to its critical role in 
              managing the metadata and synchronizing user's files. 
            - Desktop clients communicate with synchronization service to either obtain 
              updates from the cloud storage or to send files and updates to the cloud
              storage and potentially users. 
            - If a client was offline for a period, it polls the system for new updates 
              as soon as they come online. 
            - When the synchronization service receives an udpate request, it checks
              with the metadata database for consistency and then proceeds with the update. 
            - Subsequentially, a notification is sent to all subscribed users or devices to
              report the file update. 
        * The service should be designed in such a way to transmit less data between 
          clients and the cloud storage to acheive better response times. 
            - Service should employ a differencing algorithm to reduce the 
              amount of data that needs to be synchronized.
            - Instead of transmitting entire files from clients to the server or 
              vice versa, we can just transmit the difference between two versions 
              of a file. --> only part of file that is changed is transmitted 
                * Only part of file that is changed is transmitted 
                * Reduces bandwidth and cloud data storage for end user 
                * Divide files in 4MB chunks and will transfer modified chunks 
                  only. 
                * Server/Clients can calculate hash (i.e. SHA-256) to see whether to
                  update the local copy of a chunk or not. 
                * On server side, if we have chunk with similar hash (even from another user)
                  Then don't need to create another copy and can just use same chunk.
                  See data depulication below.
        * To be able to provide an efficient and scalable synchrnoization protocol, we
          can consider using a communication middleware between clients and the synchronization
          service.
            - The messaging middleware shoudl provide scalable message queuing and change 
              notifications to support a high number of clients using pull or push strategies. 
            - This way, multiple syncrhonization service instances can receive requests 
              from a global request Queue, and the communication middle will be able to
              balance its load. 

    - Message Queuing Service 
        * An important parot of our architecture is a messaging middleware that 
          should be able to handel a substantial number of requests. 
            - A scalable message quieing service that supports asynchronous 
              message-based communication between clients and the synchronization 
              service best fitst he requirements of our application.
            - The message queuing service supports asynchronous and loosely
              coupled message-based communication between distributed components 
              of the system.
            - The message queuing service should be able to efficiently store 
              any number of messages in a highly available, reliable, and scalable queue. 
        * The message queuing service will implement two types of queues in our system.
            - The request queue is a global queue and all clients will share it.
                * Clients requests to update the metadata database will be sent to the request 
                  queue first, from there the synchronization service will take it update metadata.
            - The response queues that correspond to individual subscribed clients 
              are responsible for delivering the update messages to each client. 
                * Since a message will be deleted from the queue once received by a 
                  client, we need to create separate Rseponse Queues for each 
                  subscribed client to share update messages. 


            
            |-----------> request queue [1,2,3] ------------------|
            |                                                     |
            |                                                     |
            |                                                     v
            |<--client<--- response queue [1,2,3] <-------------synchronization service 
            |                                                  |     |
            |<--client<--- response queue [1,2,3] <------------|     |
            |                                                  |     |
            |<--client<--- response queue [1,2,3] <------------|     v
                                                                    metadata db

    - Cloud/Block Storage 
        * Stores chunks of files uploaded by the users. Clients directly interact with the 
          storage to send and receive objects from it. 
        * Separation of metadata from storage enables us to use storage either in the cloud
          or in-house 
        * See figure dropbox_figure1.png

                     notification server <----synch queue--- metadata servers  <---- metadata db 
                        |                                          ^    ^                  |
                        |                                          |    |                  |
                        |                                          |    |                  v
        client----->    V                                          |    |-------------- metadata cache server 
        client-----> internet -----------------------------------> LB 
        client----->     |                                          ^    |------------------storage cache server 
                         |                                          |    |                        ^
                         |                                          |    |                        |
                         |                                          |    V                        |
                         |--------------------------------------> Block server -----------> Block/cloud storage 


* File Processing Workflow 
    - The sequene below shoes the interaction between the components of the application
      in a scenario when client A updates a file that is shared with client B and C, so that
      they should receive the update too. 
    - If the other clients are nto online at the time of the update, the message 
      queuing service keeps the update notification in separate response queues
      for them until they come online later. 
    - Sequence
        * Client A uploads chunks to cloud storage 
        * Client A updates metadata and commits changes 
        * Client A gets confirmation and notifications are sent to client B and C 
          about the changes. 
        * Client B and C receive metadata changes and download updated chunks 


* Data Deduplication
    - Remove duplicate copies of to improve storage utilization
    - Can also be applied to network data transfers to reduce the number of bytes 
      transferred 
    - For each chunk, calculate hash, and see if that hash already exists in our storage 
    - Two ways to implement Deduplication
        * Post-Process Deduplication 
            - New chunks are saved in storage and another process analyzes the data 
              looking for deduplication 
            - Benefit would be that the client does not need to wait for the hash
              calculation or for the lookup to complete ensuring there is no degradation
              in storage performance from the client side 
            - Drawbacks include
                * storing duplicated data (even for a short period of time)
                * transferring unnecessary data
        * In-Line Deduplication 
            - Alteratively, deduplication hash calculations can be done in real-time 
              as the clients are entering data on their device. 
            - If system identifies a chunk that has already been stored, only a reference 
              to the existing chunk will be added in the metadata rather than a full copy
              of the chunk. 
            - Approach will give us optimal network and storage usage. 
        
* Metadata partitioning 
    - To scale our metadata DB, wen eed to partition it so that it can store information
      about millions of users and billions of files/chunks. We need to come up with a 
      partitioning scheme that would dive and store our data in different DB servers. 
    - Vertical Partitioning 
        * We can partition our database in such a way that we store tables related to
          one particular feature on one server. 
        * For example, we could store all user related things on a table on one server 
          and all file/chunk related things on a table in another database server. 
        * Issues:
            - Will we have scale issues?
                * What if we have trillions of chunks to be stored and our database 
                  cannot support storing such a huge number of records? 
                * How could we further partition our table?
            - Joining two tables in two separate databases can cause performance consistency 
              issues. How frequently do we have to join user and file tables?
    - Range based partitioning 
        * What if store files/chunks in separate partitions based on the first 
          letters of the filepath?
            - In that case, we can save all the files starting with the letter 'A' in one 
              partition and those that with letter 'B' in another partition, etc.
            - Can even combine less frequently occuring values in to one database partition.
            - We should come up with partitioning scheme statically so that we can always 
              store-find a file in a predictable manner.
        * Main Problem  
            - Can lead to unbalanced servers. For example, if we decide to put all files 
              starting with the letter 'E' into a DB partition, and later realize 
              that we have too many files that start with the letter 'E' to such an 
              extent that we can't fit into one DB partition. 
    - Hash-Based Partitioning 
        * In this scheme, we take a hash of the object we are storing and based on this
          hash we figure out the DB partition to which this object should go. 
        * In our case, we can take a hash of the 'FileID' of the field object we are 
          storing to determine the partition the file will be stored in. 
        * Hashing function can randomly disribute objects into different partitions,
          i.e. hashing function can always map any ID to a number between
          [1...256] and this number would be the partition we will store our object.
        * Approach can lead to overlaoded partitions which can be solved using consistent
          hashing. 


* Caching 
    - We can have two kinds of caches in our system. 
        * To deal with hot files/chunks we can introduce a cache for block storage. 
            - Can use an off-the-shelf solution like Memcached that can store 
              whole chunks with its respective IDs/Hashes and then the Block
              servers can quickly check if the cache has desired chunk before 
              hitting the block storage. 
            - Based on clients' usage patterns we can determine how many 
              cache servers we need. A high-end commerical server can have 144 GB
              of memory; one such server can cache 36k chunks. 
            - Cache replacement policy?
                * LRU 


* Load Balancer (LB)
    - Can add load balancer at two places in our system:
        * Between clients and block servers 
        * Between clients and metadata servers 
    - Can use round robin policy approach that distributes incoming requests 
      equally among backend servers 
        * Easy to implement and does not introduce overhead 
        * If server dies, LB will take it ouf the rotation and will stop sending 
          traffic to it. 
        * Problem with round robin is that it does not take server load into 
          consideration. 
            - If server is overloaded or slow, the LB will not stop sending 
              new requests to that server. 
            - To handle this, a more intelligenet LB solution can be placed 
              that periodically queries backend server about their load and 
              adjusts traffic based on that. 


* Security, Permissions, and File Sharing 
    - One of the primary concerns users have with storing their data in the cloud 
      is the privacy and security of their data, especially since in our 
      system users can share their files with other users or even make them
      public to share with everyone. 
    - To handle this, we will be storing the permissions of each file in our metadata
      DB to reflect what files are visible or modifiable by any user. 








        