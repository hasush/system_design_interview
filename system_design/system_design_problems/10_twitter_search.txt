* Designing Twitter Search
    - One of the largest social network services where users can shares 
      photos, news, and text-based messages.
    - Will design a service that store and search user tweets 


* What is Twitter Search?
    - Twitter users can update their status whwenever they like.
    - Each status (called tweet) consists of plain text and our goal 
      is to design a system that allows searching over all the user tweets.


* Requirements and Goals of the System 
    - Assume 1.5e9 total users and 800e6 DAU (Daily Active Users)
    - 400e6 new tweets/day 
    - 300 bytes/tweet 
    - Assume 500e6 searches/day 
    - Search query will consist of multiple words combined with AND/OR 
    - Need to design a system that can efficiently store and query tweets 


* Capacity Estimation and Constraints 
    - Storage Capacity 
        * Data Per Day 
            - 400e6 * 300 bytes = 120e9 = 120 GB/day 
        * Storage Per Second 
            - 120 GB/day * 1day/(24*3600) < 120 GB/ (25*3000) = 120/75 MB < 2 MB/sec (1.38 actual)


* System APIs 
    - We can SOAP and REST APIs to expose the functionality of our service
    - search(api_dev_key, search_terms, maximum_results_to_return, sort, page_token)
        * api_dev_key (string): The API developer key of a registered account. 
                                This will be used to, among other things, throttle 
                                users based on their allocated quota.
        * search_terms (string): A string containing the search terms.
        * maximum_results_to_return (number): Number of tweets to return.
        * sort (number): Optional sort mode: Latest first (0 - default), 
                         Best matched (1), Most liked (2).
        * page_token (string): This token will specify a page in the result set 
                               that should be returned.
    - Returns (JSON):
        * A JSON containing information about a list of tweets matching the search
          query.
        * For each result entry can have the user ID & name, tweet text, tweet ID,
          creation time, number of likes, etc.


* High Level Design 
    - At the high level, we need to store all the tweets in a database and 
      also build an index that can keep track of which words appears in which 
      tweet.
    - This index will help us quickly find tweets that hte users are trying 
      to search for. 


    clients --- update status ---> application server ----------> storage server 
            --- search status --->      |
                                        |
                                        |
                                        |
                                        v 
                                    index server 


* Detailed Componenet Design 
    - Storage 
        * We need to store 120 GB of new data every day. 
        * Given this huge amount of data, we need to come up with a data 
          partitioning scehem that will be efficiently distributing the data 
          onto multiple servers. 
        * Five year storage 
            - 120 GB/day * 365 * 5 ~= 120 * 400 * 5 = 120 * 2000 = 240 TB
        * If we never want to be more than 80% full at any time, we approximately 
          will need 240 * 5/4 = 300 TB 
        * Want to keep extra copy of tweets for fault tolerance
            --> 600 TB storage 
        * If modern server can hold up to 4 TB of data, we would need 150 servers 
          to hold required data for 5 years. 
        * Start with a simplistic design where we store the tweets in a MySQL
          database.
            - We can assume that we store the tweets in table having two 
              columns: TweetID and TweetText 
            - Let's assume we partition our data based on TweetID 
            - If our TweetIDs are unique system-wide, we can define a hash 
              function that can map a TweetID to a storage server where we can store that 
              tweet object. 
        * How can we create system-wide unique TweetIDs?
            - If we are getting 400e6 new tweets each day, then how many tweet
              objects can we expect in 5 years?
                * 400e6 * 365 * 5 < 400e6 * 2000 = 800e9 ~ 1e12 ~ 40 bits = 5 bytes 
                  to identify TweetIDs uniquely 
                * Assume we have a service that can generate a unique TweetID whenever 
                  we need to store an object (i.e. see 6_twitter.txt)
                  We can feed the TweetID to our hash functions to find the storage 
                  server and store our tweet object there. 
        * Index 
            - What should our index look like?
                * Since our tweet queries will consist of words, let's build the index 
                  that can tell us which word comes in which tweet object. 
                * Let's first estimate how big our index will be.
                    - If we want to build an index for all the English words and some 
                      famous nouns like people names, city names, etc., and if we 
                      assume that we have around 300K English words and 200K nouns, then we 
                      will have 500K total words in our index. 
                    - Let's assume that the average length of a word is five characters.
                    - If we are keeping our index in memory, we need 2.5 MB of memory to store 
                      all the words. 
                      500K * 5 * 1 byte = 2.5 MB 
                    - Let's assume that we want to keep the index in memory for all the tweets from
                      only past two years. 
                        * Will be getting 730e9 tweets in 5 years, this will give us 292e9
                          tweets in two years. 
                        * Given that each TweetID will be 5 bytes, how much memory will we need 
                          to store all the TweetIDs?
                            292e9*5 = 1450 GB 
                    - So our index would be like a big distributed hash table, where 'key' would 
                      be the word and 'value' will be a list of TweetIDs of all those tweets which 
                      contain that word.
                    - Assuming on average we have 40 words in each tweet and since we will not be 
                      indexing prepositions and other small words like 'the' 'an' 'and' etc. lets assume
                      we will have around 15 words in each tweets that need to be indexed.
                    - This means each TweetID will be stored 15 times in our index.
                    - Total memory we will need to store our index
                        1460 GB * 15 + 2.5MB ~= 1500 GB * 15 = 22.500 TB
                    - Assuming high-end server has 144 GB of memory, we would need ~150 such servers to hold
                      our index. 
        * Sharding based on words 
            - While building our index, we will iterate through all the words of a tweet 
              and calculate the hash of each word to find the server where it should be indexed. 
            - To find all tweets containing a specific word we have to query only the server 
              which contains this word 
                * What if a word becomes hot? Then there will be a lot of queries on the 
                  server holding that word. This high load will affect the performance of 
                  our service. 
                * Over time, some words can up storing a lot of TweetIDs compared to others, therefore, 
                  maining a uniform distribution of words while tweets are growing is quite tricky.
            - To recover from these situations, we either have to repartitino our data or use consistent 
              hashing. 
        * Sharding based on the tweet object 
            - While storing, we will pass the TweetID to our hash function to find the server
              and index all the words of the tweet on that server.
            - While querying for a particular word, we have to query all the servers and each 
              server will return a set of TweetIDs.
            - A centralized server will aggregate these results to return them to the user 
            - See twitter_search_figure1.png 

            client ---> LB ---> application servers 
                                       |
                                       |
                                       |
                                       v 
                               aggregtate servers  <----> Database Shards 
                                       |                        ^
                                       |                        |
                                       |                        |
                                       |                        |
                                       v                        v
                                 index servers  <---------> index-builder servers 


* Fault Tolerance 
    - What will happen when an index server dies? We can have a secondary replica 
      of each server and if the primary server dies it can take control after the failover.
      Both priamry and secondary servers will have the same copy of the index. 
    - What if both priamry and secondary servers die at the same time?
        * We have to allocate a new server and rebuild the same index on it.
        * How do we do that?
            - We don't know what words/tweets were kept on this server.
            - If were using "Sharding based on the tweet object', the brute-force 
              solution would be to iterate through the whole database and filter
              TweetIDs using our hash function to figure out all the required 
              tweets that would be stored on this server. 
            - This would be inefficient and also during this time when ther server was being 
              rebuilt we would not be able to serve any query from it, thus missing 
              some tweets that should have been seen by the user. 
    - How can we efficiently retrieve a mapping between tweets and the index server?
        * We will have to build a reverse index that will map all the TweetID to their 
          index server.
        * Our index-builder server can hold this information.
        * We will need to build a HashTable where the 'key' will be the index server
          number and the 'value' will be a HashSet containing all the TweetIDs being 
          being kept at that index server. 
        * Notice that we are keeping all the TweetIDs in a HashSet; this will enable 
          us to add/remove teweets from our index quickly.
        * So now, whenever an index has to rebuild itself, it can simply ask the
          Index-Builder server for all the tweets it needs to store and then fetch 
          those tweets to build the index. 
        * This approach will surely be fast.
        * We should also have a replica of the Index-Builder server for fault tolerance.


* Cache 
    - To deal with hot tweets we can introduce a cache in front of our database. 
    - We can use Memcached, which can store all such hot tweets in memory.
    - Application servers can quickl if the cache has the tweet before hitting the backend. 
    - Based on clients' usage pattersn, we can adjusth ow many cache servers we need. 
    - LRU is a good policy


* Load Balancing (LB) 
    - We can add a load balancing layer at two places in our system 
        * Between Clients and Applications servers
        * Between application servers and backend servers 
    - Initially a simple round robin approach can be adopted that distributes 
      incoming requests equally among backend servers 
    - This LB is simple to implement and does not introduce any overhead 
    - Another benefit of this approach is LB will take dead servers out of the rotation 
      and will stop sending any traffic to it. 
    - A problem with Round Robin LB is it won't take server load into consideration.
    - If a server is overloaded or slow, the LB will not stop sending new requests
      to that server.
    - To handle this, am ore intelligent LB solution can be placed that periodically 
      queries the backend server about their load and adjust traffic based on that. 


* Ranking 
    - How about we want to rank the earch result by social graph distance, popularity,
      relevance, etc.?
    - Let's assume we want to rank tweets by popularity, like how many like or comments
      a tweet is getting, etc.
    - In such a case, our ranking algorithm can calculate a 'popularity number'
      based on the number of likes, etc. ...and store it with the index 
    - Each partition can sort the results based on this popularity number 
      before returning results to the aggregator server 
    - The aggregator server combines all these results, sorts them based on the popularity 
      number and sends the top results to the user. 

            
                    
                