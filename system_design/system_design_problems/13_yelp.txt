* Designing Yelp (Proximity Server)
    - Let's design a Yelp like service where users can search for nearby places like
      restaurants, theaters, or shopping malls, etc. and can also add/view reviews of
      places. 


* Why Yelp or Proximity Server?
    - Proximity servers are used to discover nearby attractions like places, events,
      etc.


* Requirements and Goals of the System 
    - What do we wish to achieve from a Yelp like service?
        * Our service will be storing information about different places 
          so that users can perform a search on them.
        * Upon querying, our service will return a list of places 
          around the user. 
    - Functional Requirements 
        * Users should be able to add/delete/update places 
        * Given their location (latitude/longitude), users should be able to 
          find nearby places within a given radius
        * Users should be able to add feedback/review about a place.
          The feedback can have pictures, text, and a rating.
    - Non-Functional Requirements 
        * Users should have a real-time search experience with minimum latency.
        * Our service should support a heavy search load. There will be a lot 
          of search requests compared to adding a new place. 


* Scale estimation 
    - Let's build our system assuming that we have 500e6 places and 
      100e3 queries/sec (QPS).
    - Let's assume a 20% growth in the number of places and QPS each year. 


* Database Schema 
    - Each place can have the following fields 
        * LocationID (8 bytes)
        * Name (256 bytes)
        * Latitude (8 bytes)
        * Longitude (8 bytes)
        * Description (512 bytes)
        * Category (1 byte) (i.e. coffee shop, restaraunt, theater, etc.)
    - Although a four byte number can unique identify 500e6 locations, with future 
      growth in mind, we will go with 8 bytes for LocationID 
    - Total Size 
        8 + 256 + 8 + 8 + 512 + 1 = 793 bytes 

    - Also need to store reviews, photos, and ratings of a place. We can have a 
      separate table to store reviews for places.
        * LocationID (8 bytes)
        * ReviewID (4 bytes)
        * ReviewText (512 bytes)
        * Rating (1 byte)

    - Similary, can have a separate table to store photos for places.
        * LocationID (8 bytes)
        * Photo (64kB)


* System APIs 
    - We can have SOAP or REST APIs to expose the functionality of our service. 
    - The following could be the definition of the API for searching 
    - search(api_dev_key, search_terms, user_location, radius_filter,
             maximum_results_to_return, category_filter, sort, page_token)
        * Parameters:
        * api_dev_key (string): The API developer key of a registered account. 
          This will be used to, among other things, throttle users based on their 
          allocated quota.
        * search_terms (string): A string containing the search terms.
        * user_location (string): Location of the user performing the search.
        * radius_filter (number): Optional search radius in meters.
        * maximum_results_to_return (number): Number of business results to return.
        * category_filter (string): Optional category to filter search results, 
          e.g., Restaurants, Shopping Centers, etc.
        * sort (number): Optional sort mode: Best matched (0 - default), 
          Minimum distance (1), Highest rated (2).
        * page_token (string): This token will specify a page in the result set 
          that should be returned.

        * Returns: (JSON)
        * A JSON containing information about a list of businesses matching the 
          search query. Each result entry will have the business name, address, 
          category, rating, and thumbnail.


* Basic System Design and Algorithm 
    - At a high level, we need to store and index each dataset described above 
      (places, reviews, etc.) 
    - For users to query this massive database, the indexing should be read efficient,
      since while searching for the nearby places users expect to see the 
      results in real-time. 
    - Given that the location of a place doesn't change that often, we don't 
      need to worry about frequent updates of the data. 
    - As a contrast, if we intend to build a service where objects do change 
      their location frequently, i.e. people or taxis, then we might come 
      up with a different design.
    - Let's see what are different ways to store this data and also find out 
      which method will suit best for our use cases 
    - SQL Solution 
        * One simple solution could be to store all the data in a databse like MySQL
        * Each place will be stored in a separate row, uniquely identified by 
          LocationID.
        * Each place will have its longitude and latitude stored separately in 
          two different columns, and to perform a fast search, we should have indexes 
          on both of these fields. 
        * To find all the nearby places of a given location (X,Y) within a radius
          'D', we cna query like this 
        
            Select * from Places where Latitude between X-D and X+D and Longitude 
            between Y-D and Y+D 
        
        * The above query is not completey accurate, as we know that to find the distance
          between two points we have to use euclidean distance
        * How efficient would this query be?
            - We have estimated 500e6 places to be stored in our service.
            - Since we have two separate index, each index can return a huge list of 
              places and performing an intersection on those two lists won't be 
              efficient.
            - Another way to look at this problem si that could be too many locations 
              between 'X-D' and 'X+D' and Similary between 'Y-D' and 'Y+D'. If we 
              could shorten these lists, it can improve the performance of our query.     
    - Grids Solution 
        * We can divide the whole map into smaller grids to group locations into 
          smaller sets.
        * Each grid will store all the places residing within a specific range 
          of longitude and latitude.
        * This scheme would enable us to query only a few grids to find nearby places.
        * Based on a given location and radius, we can find all the neighboring 
          grids and then query these grids to find nearby places. 
        * Let's assume that GridID (a four bye number) would uniquely identify 
          grids in our system. 
        * What would be a reasonable grid size?
            - Grid size could be equal to the distance we would like to query since 
              we also want to reduce the number of grids.
            - If the grid size is equal to the distance we want to query, then we 
              only need to search within the grid which contains the given location
              and neighboring eight grids.
            - Since our grids would be statically defined (from the fixed grid size),
              we can easily find the grid number of any location (lat/long) and 
              its neighboring grids. 
            - In the database, we can store the GridID with each location and 
              have an index on it for faster searching.
            - Now our query will look like 

                Select * from Places where Latitude between X-D and X+D 
                and Longitude between Y-D and Y+D and GridID in
                (GridID, GridID1, GridID2, ..., GridID8)
            
            - This will undoubtedly improve the runtime of our query. 
        * Should we keep our index in memory?
            - Maintaining the index in memory will imrpove the performance of our 
              service.
            - We can keep our index in a hash table where 'key' is the grid number and
              'value' is the lsit of places contained in that grid. 
        * How much memory will we need to store the index?
            - Let's assume our saerch radius is 10 miles.
            - Given that the total area of Earth is around 200e6 square miles, we will
              have 20e6 grids. 
            - We would want a four byte number to unique identy each grid and since
              LocationID is 8 bytes, we would need 4GB of memory (ignoring hash table 
              overhead) to store the index 
            
                4*20e6 + 8*500e6 ~= 4e9 = 4 GB 
            
            - This solution can still run slow for those grids that have a lot of 
              places since our places are not uniformly distributing among grids. 
            - We can ahve a thickly dense area with a lot of places and on the other 
              hand we can have areas which are sparsely populated. 
            - This problem can be solved if we dynamically adjust our grid 
              size such that whenever we have a grid with a lot of places we break it down
              to create smaller grids.
            - A couple of challenges with this approach could be
                * How to map these grids to locations 
                * How to find all the neighboring grids of a grid 
    
    - Dynamic Size Grids 
        * Let's assume we don't want to have more than 500 places in a grid
          so that we can have faster searching. 
        * So whenever a grid reaches this limit, we break it down into 
          four grids of equal size and distribute places among them.
        * This means thickly populated areas like downtown San Francisco
          will have a lot of grids, and sparsely populated are like the 
          Pacific OCean will have a large grid with places only around the coastal lines.
        * What data structure can hold this information?
            - A tree in which each node has four children can serve our purpose. 
            - Each node will represent a grid and will contain information about all
              the places in that grid.
            - If a node reaches our limit of 500 places, we will break it down 
              to create four child nodes under it and distribute places among them.
            - In this way, all the leaf nodes will represent the grids that 
              cannot be further broken down.
            - So leaf nodes will keep a list of places with them.
            - This tree stucture in which each node can have four children 
              is called a QuadTree 
        * How will we build a QuadTree?
            - We will start with one node that will reprsent the whole world in one
              grid. Since it will have more than 500 locations, we will break it down into
              four nodes and distribute locations among them.
            - We will keep repeating this process with each child node until there are no 
              nodes left with more than 500 locations. 
        * How will we find the grid for a given location?
            - We will start with the root node and search downwar to find our required 
              node/grid.
            - At each step, we will see if the current node we are visting has children.
            - If it has, we will move to the child node that contains our desired location.
              and repeat this process.
            - If the node does not have any chidlren, then that is our desired node. 
        * How will we find neighboring grids of a given grid?
            - Since only leaf nodes contain a list of locations, we can connect all leaf 
              nodes with a doubly linked list. 
            - This way we can iterate forward or backward among the neighboring leaf nodes 
              to find our our desired locations. 
            - Another appraoch for finding adjacent grids would be through parent nodes. 
            - We can keep a pointer in each node to access it parents and since each node 
              has pointers to all of its children, we can easily find siblings of a node. 
            - We can keep expanding our search for neighboring grids by going up 
              through the parent pointers.
            - Once we have nearby LocationIDs, we can query the backend database to find 
              details about those places. 
        * What will be the search workflow?
            - We will first find the node that contains the user's location.
            - If that node has enough desired places, we can return them to the user.
            - If not, we will keep expanding to the neighboring nodes (either through
              the parent pointer or doubly linked list) until either we find the 
              required number of places or exhaust our search based on hte maximum radius. 
        * How much memory will be needed to store the QuadTree?
            - For each Place, if we cache only LocationID and Lat/Long, we would need 12 GB
              to store all Places 
                3*8*500e6 = 12000e6 = 12 GB 
            - Since each grid can have a maximum of 500 places, wand we have 500e6 locations,
              we will have 1e6 grids 
                500e6/500 = 1e6 
            - This means we will have 1e6 leaf nodes and they will be holding 12 GB of location 
              data. 
            - A QuadTree with 1e6 leaf nodes will have approximately 1/3rd internal nodes 
              and each internal node will have 4 pointers (for its children). If each pointer 
              is 8 bytes, then the memory we need to store all internal nodes would be 
                1e6 * 1/3  * 4 * 8 = 4e6 ~= 10 MB 
            - So total memory would be 12.01 GB which can easily fit into modern server. 
        * How could we insert an ew plae into our system?
            - Whenever a new palce is added by a user, we need to insert into the databases 
              as well as in the QuadTree. 
            - If our tree resides one one server, it is easy to add a new Place, but 
              if the QuadTree is distributed among different servers, first we need to find 
              the grid/server of the new Place and then add it there. 


* Data Partitioning 
    - What if we have a huge number of places such that our index does not 
      fit int oa single machine's memory?
    - With 20% growth each year, we will reach the memory limit of the server in the 
      future. 
    - Also, what if one server cannot serve the desired read traffic?
    - To resolve these issues, we must partition our QuadTree! 
    - We will explore two solutions here (both of these partitioning schemes can 
      be applied to databases too)
    - Sharding Based on Regions 
        * We can divide our palces into regions (like zip codes) such that all 
          places belonging to a region will be stored on a fixed node. 
        * To store a place we will find the server through its region and 
          similarly while querying for nearby places we will ask the region 
          server that contains users location.
        * Issues 
            - What if a region becomes hot? There would be a lot of queries 
              on the server holding that region, making it perform slow.
              This will affect the performance of our service. 
            - Over time, some regions can end up storing a lot of places compared 
              to others. Hence, maintaing a uniform distribution of places 
              while regions are growing is quite difficult.
        * To recover from these situations, either we have to repartition the data 
          or use consistent hashing.
    - Sharding based on LocationID 
        * Our hash function will map each LocationID to a server where we will store 
          that place. 
        * While building our QuadTree, we will iterate through all the places 
          and calculate the has of each LocationID to find a server where it would 
          be stored. 
        * To find places near a location, we have to query all servers and 
          each server will return a set of nearby places. 
        * A centralized server will aggregate these results to return them to
          the user. 
    - Will we have different QuadTree structure on different partitions?
        * Yes, this can happen since it is not guaranteed that we will have an
          equal number of palces in any given grid on all partitions.
        * However, we do make sure that all servers have approxiamtely an equal number 
          of Places.
        * This different tree stucture on different servers will not cause any issue 
          though, as we will be searching all the neighboring grids within the given 
          radius on all partitions. 
    - Remaining chapter assumes we have partitioned our data based on LocationID 


* Replication and Fault Tolerance 
    - Having replicas of QuadTrees servers can provide an alternate to data 
      partitioning.
    - To distribute read traffic, we can ahve replicas on each QuadTree server. 
    - We can ahve a primary-secondary configuration where replicas (secondaries)
      will only serve read traffic; all write traffic will go the priamry and then 
      applied to secondaries.
    - Secondaries might not have some recently inserted places (a few milliseconds 
      delay will be there), but this could be acceptable. 
    - What will happen when a QuadTree server dies?
        * We can have a secondary replica of each server and if the primary dies,
          it can take control after the failover.
        * Both primary and secondary servers will have the same QuadTree stucture.
    - What if both primary and secondary servers die at the same time?
        * We have to allocate a new server and rebuild the same QuadTree on it.
        * How can we do that since we don't know what places were kept 
          on this server?
        * The brute force solution would be to iterate through the whole database 
          and filter LocationIDs using our hash function to figure out all the 
          required places that will be stored on this server. 
        * This would be inefficient and slow; also during the time when the server 
          is being rebuilt, we will not be able to serve any query from it, thus
          missing some places that should have been seen by users. 
    - How can we efficiently retrieve a mapping between Places and QuadTree server?
        * We will have to build a reverse index that will map all the Places to their
          QuadTree server.
        * We can have a separate QuadTree Index server that will hold this information.
        * We will need to build a HashMap where the 'key' is the QuadTree server 
          number and the 'value' is a HashSet containg all the Places being 
          kept on that QuadTree server.
        * We need to store LocationID and Lat/Long with each Place so that 
          information servers can build their QuadTrees
        * Notice that we are keeping Places' data in a HashSet as this will 
          enable us to add/remove Places from our index quickly. 
        * Whenever a QuadTree servers need to rebuild itself, it can simply 
          ask the QuadTree Index Server for all the Places it needs to store. 
        * This approach will be much faster. 
        * We should also have a repliace of the QuadTree Index server for fault 
          tolerance.
        * If a QuadTree Index server dies, it can always rebuild its index from 
          iterating through the database. 

                        Aggregation Servers      QuadTree Server
    client ---> LB ---> Aggregation Servers ---> QuadTree Server <--- Quad Tree Index ---> DataBase
                        Aggregation Servers      QuadTree Server
                        (Aggregation Layer)  (Partitioned QuadTree)


* Cache 
    - To deal with hot Places, we can introduce a cache in front of our database. 
    - We can use an off-the-shelf solution like Memcache which can store all data
      about hot places
    - Application server, before hitting the back databse, can quickl check if the cache 
      has that Place.
    - Based on the clients' usage pattern ,we can adjust how many cache servers 
      we need.
    - LRU is good policy 


* Load Balancing (LB)
    - We can add LB layer at two places in our system 
        * Between Clients and Application Servers 
        * Between Application Servers and Backend servers 
    - Can use Round Robin as this will distribute all incoming requests equal among 
      backend servers. 
        * This LB is simple to implement and does not introduce any overhead. 
        * Another benefit of this approach is if a server is dead the load balancer 
          will take it out of the rotation and will stop sending any traffic to it. 
    - A problem with Roun Robin LB is that it won't take server laod into consideration.
        * If a server is overloaded or slow, the LB will not stop sending new requests 
          to that server. 
        * To handle this, a more intelligent LB solution would be needed that periodically 
          queries backend server about their load and adjusts traffic based on that. 


* Ranking 
    - How about if we want to rank the search results not by just proximity 
      but also by popularity or relevance?
    - How can we return most popular places within a given radius?
        * Let's assume we keep track of the overall popularity of each place.
        * An aggregated number can reprsent thsi popularity in our system.
          i.e. how many stars a place gets out of ten (an average ranking).
        * We will store this number in the database as well as in the QuadTree.
        * While searching for the top 100 places within a given radius, we can ask 
          each partition of the QuadTree to return the top 100 palces with 
          maximum popularity.
        * Then the aggregator server can determine the top 100 places among all the
          places returned by different partitions. 
    - Remember that we didn't build our system to update place's data frequently. 
    - With this design, how can we modify the popularity of a place in our QuadTree?
    - ALthough we can search a place and update it popularity in the QuadTree, it 
      would take a lot of resources can affect search requests and system throughput.
    - Assuming the popularity of a palce is nto expected to reflect in the system within
      a few hours, we can decide to update it once or twice a day, especially when the 
      load on the system is a minimum. 
    - See 14_uber.txt for dynamic updates to the QuadTree 





